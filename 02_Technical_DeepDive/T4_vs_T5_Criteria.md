# 🔬 T4(구조 차용) vs T5(독자 설계) 기술적 판별 기준
> **Technical Criteria for Distinguishing Adopted vs Native Architectures**

같은 자체 학습(Pre-training) 모델이라도, T4와 T5는 엔지니어링의 차원이 다릅니다. 명확한 구분을 위해 설계의 본질, 구현의 깊이, 언어 처리 효율, 지식의 기원이라는 4가지 기술적 판별 기준을 제시합니다.

---

## 1. 변경의 본질: Config 튜닝인가, Topology 재설계인가?
가장 본질적인 차이는 변화가 **정량적(Quantitative)**이냐 **정성적(Qualitative)**이냐에 있습니다.

### **T4-2 (양적 변형 - Scaling)**
* **상세:** 기존 아키텍처의 Hyperparameter를 조정하는 데 그칩니다. `config.json` 파일 내의 레이어 깊이, 임베딩 차원, 어텐션 헤드 수와 같은 수치를 변경합니다.
* **기술적 함의:** 30층 건물을 50층으로 증축하는 것과 같습니다. 건물의 하중을 견디는 공법이나 내부 배관 구조는 원본(Llama, Mistral 등)과 동일합니다.

### **T5 (질적 변형 - Architecting)**
* **상세:** 모델의 **연산 그래프(Computational Graph)**와 **토폴로지(Topology)** 자체를 변경합니다.
    * *예: MHA(Multi-Head) → MLA(Multi-Latent) 교체, Pre-Norm/Post-Norm 위치 변경, 활성화 함수 재정의 등*
* **기술적 함의:** 데이터 흐름(Data Flow)을 바꿉니다. 입력이 출력으로 나갈 때까지의 계산 순서와 경로를 재설계하여, 연산 효율과 정보 보존력을 근본적으로 변화시킵니다.

---

## 2. 구현의 깊이: 코드 호환성(Compatibility)과 의존성
"표준 라이브러리 위에서 그냥 돌아가는가?"는 주권을 판별하는 중요한 기준입니다.

### **T4 (플랫폼 종속형)**
* **특징:** HuggingFace의 `LlamaForCausalLM`이나 `MistralForCausalLM`과 같은 표준 클래스를 상속받아 사용합니다.
* **기술적 증거:** 구동 코드(`modeling.py`)를 뜯어보면, 핵심 연산 로직이 원본 모델과 변수명까지 유사합니다. 향후 원본 아키텍처의 취약점이나 라이선스 이슈 발생 시 리스크를 상속받습니다.

### **T5 (코드 독립형)**
* **특징:** 기존 로더와 호환되지 않아 **독자적인 모델링 코드(Custom Modeling Code)**를 작성해야 합니다.
* **기술적 증거:** `trust_remote_code=True` 옵션 없이는 구동되지 않으며, 내부적으로 독자 정의된 클래스와 함수들이 작동합니다. 이는 글로벌 표준 생태계의 변화와 무관하게 독자 생존이 가능한 **기술적 격리 상태**를 의미합니다.

---

## 3. 언어 기관: 확장(Expansion)인가, 원천 구축(Native)인가?
토크나이저(Tokenizer)는 AI가 세상을 읽는 눈입니다.

### **T4 (확장형)**
* **방식:** 기존 모델(Llama 3 등)의 토크나이저를 베이스로 삼고, 그 뒤에 한국어 단어 수만 개를 덧붙이는 방식을 씁니다.
* **한계:** 기존 영어 중심의 Embedding Space 구석에 한국어를 셋방살이 시키는 구조입니다. 한국어 문장의 압축률(Compression Rate)이 떨어져 운영 비용(TCO)이 증가합니다.

### **T5 (네이티브형)**
* **방식:** 기존 토크나이저를 쓰지 않고, 무작위 초기화(Random Initialization) 상태에서 한국어 코퍼스를 **Train from Scratch** 시킵니다.
* **강점:** 한국어의 교착어적 특성(조사, 어미 분리)을 완벽하게 반영하여 토큰 효율을 극대화합니다. 데이터 처리 비용의 주권을 확보한 것입니다.

---

## 4. 지식의 기원: 증류(Distillation)인가, 학습(Pre-training)인가?

### **T4 (지식 증류)**
* **방식:** GPT-4나 Claude 같은 최상위 모델(Teacher Model)이 생성한 합성 데이터를 학습하거나, 그들의 출력 확률(Logits)을 모사합니다.
* **함의:** 효율적일지 몰라도, 지식의 편향성과 논리 구조가 Teacher 모델에 종속되는 **Degraded Copy**에 가깝습니다.

### **T5 (원천 학습)**
* **방식:** 타사 모델의 지식을 빌리지 않고, 직접 큐레이션한 **원천 데이터(Raw Corpora)**의 통계적 패턴을 스스로 학습합니다.
* **함의:** 비용이 많이 들지만, 이를 통해서만 우리의 문화적 맥락(Context)과 가치관이 내재화된 고유한 지능을 탄생시킬 수 있습니다.
